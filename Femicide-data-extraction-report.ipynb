{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcd7f2-9596-49bc-b430-9425dfe6f2a1",
   "metadata": {},
   "source": [
    "# Scraping and Analysing Femicide Data in Kenya (2016–2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794eabaa-e2a1-4629-8745-94fe8dea5ef0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Femicide has become a common term in news and social media, and I wanted to understand the scale of the issue through actual data. In searching for a reliable source, I came across an online repository by <a href=\"https://www.africadatahub.org/femicide-kenya\">Africa Data Hub</a> that compiles femicide cases reported in Kenya.\n",
    "\n",
    "The dataset includes over 700 cases of women killed between January 2016 and December 2024, drawn from verified news articles and court records. Each entry is linked to a source, adding transparency and credibility.\n",
    "\n",
    "In this project I aim to extract, structure, and analyze the data to better understand patterns in the reported killings—who the victims were, where incidents happened, who the suspects were, and what outcomes followed. While the data may not be exhaustive, it provides a good starting point for discussing the scope and nature of femicide in Kenya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a98ae-1011-48d9-8ef1-fd2b44ec0efb",
   "metadata": {},
   "source": [
    "## Tools and libraries\n",
    "\n",
    "- **Selenium:** Used to automate browser interactions for scraping dynamic content from the target webpage.\n",
    "\n",
    "\n",
    "- **Pandas:** Employed to structure, clean, and export the scraped data for analysis.\n",
    "\n",
    "\n",
    "- **CSV:** Format used to store the cleaned dataset for further use and sharing.\n",
    "\n",
    "\n",
    "- **Visual Studio Code (VS Code):** Main development environment for writing and debugging the scraping script.\n",
    "\n",
    "\n",
    "- **Git:** Used for version control.\n",
    "\n",
    "\n",
    "- **Python:** Primary programming language used to build the scraper and handle data processing.\n",
    "\n",
    "\n",
    "- **ChromeDriver:** Bridge between Selenium and the Chrome browser, allowing automated control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa219fb5-75df-47a7-a92c-2ef14a89cec4",
   "metadata": {},
   "source": [
    "## Scraping Logic\n",
    "\n",
    "Each record on the Africa Data Hub Femicide Database is presented in a **card-style** layout. However, these cards only display limited information by default. To access full details such as name, age, location, date, suspect, verdict time, and source article, a user must click on each card, which triggers a modal popup revealing this additional data.\n",
    "[photo]\n",
    "\n",
    "To scrape all relevant information, I needed to replicate this interactive behaviour: \n",
    "- clicking each card\n",
    "- extracting the content from the modal, and\n",
    "- then closing it before moving to the next.\n",
    "\n",
    "Due to the dynamic and interactive nature of the site, I selected Selenium for this task. Selenium allows for automated browser control, enabling interaction with JavaScript-rendered elements like modals and scrollable lists.\n",
    "\n",
    "To implement this logic, I developed the following core scraping script:\n",
    "[code]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d330e0e7-1447-4d8f-b7f0-d6032a1b7f18",
   "metadata": {},
   "source": [
    "### Scraping Challenges Encountered and Solutions Implemented\n",
    "\n",
    "Despite successfully interacting with the site in a browser, the initial version of the scraper was not producing the desired results. Several issues arose during implementation:\n",
    "\n",
    "**1. Cookie Modal Obstruction:**\n",
    "\n",
    " The scraper failed to click on any of the victim cards initially. On inspection, this was due to the cookie consent modal overlapping the clickable elements.\n",
    "\n",
    " **Solution:** I introduced a command to locate and click the “Accept Cookies” button before attempting any other actions.\n",
    " [photo]\n",
    "\n",
    "**2. Empty or Missing Data:**\n",
    "\n",
    "Even after clicking the cards, some of the modal text fields we were trying to access were returning empty strings. This was due to the data not loading fast enough.\n",
    "\n",
    " **Solution:** I applied an implicit wait, allowing time for the modal text values to render before extraction.\n",
    " \n",
    "**3.Unclickable Close Button:**\n",
    "\n",
    "Attempting to close the modal using the standard `.click()` method raised a `ElementNotInteractableException`. This was because an invisible backdrop layer was overlapping the close button, making it visually present but unreachable to Selenium.\n",
    "\n",
    "**Solution:** I enforced a JavaScript click using `driver.execute_script(...)`, which allows Selenium to directly trigger a DOM click, regardless of visual obstructions.\n",
    "\n",
    "**4. Only 124 of 704 Records Extracted:**\n",
    "\n",
    "Although the page showed 704 victims, the scraper was initially retrieving only 124 records. This suggested that not all card elements were being loaded at once.\n",
    "\n",
    "**Solution:** I implemented a scrolling function to scroll to the bottom of the page repeatedly until no new cards were loaded. This ensured that all dynamic elements were visible in the DOM before scraping began.\n",
    "\n",
    "**5. Scraping Halted at Record 691:**\n",
    "\n",
    "After incorporating scrolling, the counter correctly identified 704 card slots. However, the process failed at record 691 due to an `ElementNotInteractableException.`.\n",
    "\n",
    "Upon visual inspection of the webpage, I discovered that two of the card slots were empty. These caused Selenium to attempt interactions with elements that weren’t actually there.\n",
    "\n",
    "**Solution:** I verified that only 690 valid records were written to the CSV, which aligned with expectations after subtracting the 2 empty slots.\n",
    "\n",
    "To recover the remaining records (cards `692–702`), I devised a follow-up strategy:\n",
    "- I adjusted the scraping loop to iterate from the last card backwards, limiting the run to the last 11 items.\n",
    "\n",
    "- This secondary script used the same extraction logic and was designed to append new entries to the existing CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696d1e3-cd8e-447b-b6ef-40531a552596",
   "metadata": {},
   "source": [
    "## Saving Extracted Data\n",
    "\n",
    "Once data was successfully extracted from each modal, the next step was to store it in a structured format for further analysis.\n",
    "\n",
    "To achieve this, I used Python’s `csv` and `pandas` libraries to:\n",
    "- Create a new CSV file (`femicide_data.csv`)\n",
    "- Define and write column headers (`Name`, `Age`, `Location`, `Date`, `Suspect`, `Verdict_time`, `Source_url`)\n",
    "- Insert each record row-by-row within the main scraping loop\n",
    "\n",
    "This setup ensured that even if the scraping was interrupted, each completed record was already saved to file.\n",
    "\n",
    "### Appending Remaining Records\n",
    "\n",
    "As mentioned earlier, the original run captured only 690 of the 702 valid records (due to two empty cards). To avoid rerunning the entire script and risking duplicated data:\n",
    "- I modified the scraper to target the last 11 cards only\n",
    "- The new script opened the existing CSV in append mode\n",
    "- Extracted details were then written directly into the same CSV without overwriting existing content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa62321-beb2-412a-a87c-c21e60ffe689",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "With the raw data successfully scraped and stored in a structured CSV format, the following steps are planned to complete the project pipeline:\n",
    "\n",
    "- **Load the CSV data** into a pandas DataFrame for further manipulation\n",
    "- **Clean the dataset** to handle any missing values, inconsistent entries, or formatting issues\n",
    "- Perform high level **Exploratory Data Analysis (EDA)** to uncover trends, common patterns, and demographic insights to better understand the scale and nature of femicide cases in Kenya\n",
    "- Import the cleaned data into a **visualization tool** (e.g., Power BI or Tableau) to create an interactive dashboard for clearer presentation and easier interpretation\n",
    "\n",
    "These next steps will transform the raw scraped data into meaningful insights and help communicate findings in a more accessible and compelling way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87796a3-3a61-4aba-af2b-bf6f1f374c78",
   "metadata": {},
   "source": [
    "## Future Improvements\n",
    "\n",
    "While the current project accomplishes its primary goal of extracting structured femicide data, there are several areas for enhancement to increase robustness, maintainability, and automation:\n",
    "- **Code Modularity:** Break the scraper into smaller, reusable functions and possibly wrap it into a class structure to improve readability and facilitate debugging or testing.\n",
    "- **Error Handling:** Introduce structured try-except blocks to handle common scraping issues like element not found, stale references, timeouts, or click interceptions—preventing unexpected crashes during execution.\n",
    "- **Automated Updates:** Develop a scheduled pipeline using tools like cron, Windows Task Scheduler, or Airflow to re-run the scraper periodically and check for new records on the site. (Delta Extraction).\n",
    "- **Data Validation:** Add checks to ensure that extracted fields follow expected formats (e.g., dates, names), and flag or log anomalies.\n",
    "- **Logging System:** Implement logging (e.g., with Python’s logging module) to track scraping progress, errors, and performance metrics over time.\n",
    "- **Headless Browser Optimization:** Use headless mode in production environments to reduce resource usage, while retaining the option for full browser mode during debugging.\n",
    "- **Cloud Deployment:** Host the scraper on a cloud function (e.g., AWS Lambda, Google Cloud Functions) or containerize it using Docker for scalability and portability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573226e0-3b99-4352-be7d-4a672afc99df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
